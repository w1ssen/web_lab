# Webä¿¡æ¯å¤„ç†ä¸åº”ç”¨ lab1

## ç¬¬ä¸€é˜¶æ®µï¼šçˆ¬è™«&æ£€ç´¢

### ä¸€.çˆ¬è™«

#### å®éªŒè¦æ±‚ï¼š

1.å¯¹äºç”µå½±æ•°æ®ï¼Œè‡³å°‘çˆ¬å–å…¶åŸºæœ¬ä¿¡æ¯ã€å‰§æƒ…ç®€ä»‹ã€æ¼”èŒå‘˜è¡¨ï¼›é¼“åŠ±æŠ“å–æ›´å¤šæœ‰ç”¨ä¿¡æ¯ï¼ˆå¯èƒ½æœ‰ç›Šäºç¬¬2é˜¶æ®µçš„åˆ†æï¼‰ï¼›

2.å¯¹äºä¹¦ç±æ•°æ®ï¼Œè‡³å°‘çˆ¬å–å…¶åŸºæœ¬ä¿¡æ¯ã€å†…å®¹ç®€ä»‹ã€ä½œè€…ç®€ä»‹ï¼›é¼“åŠ±æŠ“å–æ›´å¤šæœ‰ç”¨ä¿¡æ¯ï¼ˆå¯èƒ½æœ‰ç›Šäºç¬¬2é˜¶æ®µçš„åˆ†æï¼‰ï¼›

3.çˆ¬è™«æ–¹å¼ä¸é™ï¼Œç½‘é¡µçˆ¬å–å’Œ API çˆ¬å–ä¸¤ç§æ–¹å¼éƒ½å¯ï¼Œä»‹ç»ä½¿ç”¨çš„çˆ¬è™«æ–¹å¼å·¥å…·ï¼›

4.é’ˆå¯¹æ‰€é€‰å–çš„çˆ¬è™«æ–¹å¼ï¼Œå‘ç°å¹¶åˆ†æå¹³å°çš„åçˆ¬æªæ–½ï¼Œå¹¶ä»‹ç»é‡‡ç”¨çš„åº”å¯¹ç­–ç•¥ï¼›

5.é’ˆå¯¹æ‰€é€‰å–çš„çˆ¬è™«æ–¹å¼ï¼Œä½¿ç”¨ä¸åŒçš„å†…å®¹è§£ææ–¹æ³•ï¼Œå¹¶æäº¤æ‰€è·å–çš„æ•°æ®ï¼›

6.è¯¥é˜¶æ®µæ— è¯„æµ‹æŒ‡æ ‡è¦æ±‚ï¼Œåœ¨å®éªŒæŠ¥å‘Šä¸­è¯´æ˜çˆ¬è™«ï¼ˆåçˆ¬ï¼‰ç­–ç•¥å’Œè§£ææ–¹æ³•å³å¯ã€‚

#### å®éªŒæ­¥éª¤

##### 1.çˆ¬å–é¡µé¢ï¼ˆ+åçˆ¬åº”å¯¹æªæ–½ï¼‰

åœ¨part1/web_scraper.pyæ–‡ä»¶ä¸­ä½¿ç”¨ python çš„ requests åº“ï¼Œç›´æ¥è¯·æ±‚å¯¹åº”çš„ urlæ¥è·å– html çš„å†…å®¹ã€‚

çˆ¬å–è¿‡ç¨‹ä¸­é€šè¿‡ç”Ÿæˆéšæœºipï¼Œæ‰‹åŠ¨æ·»åŠ æŠ¥å¤´ï¼Œæ·»åŠ cookiesï¼Œçˆ¬å–ä¼‘æ¯æ¥è§„é¿å¹³å°åçˆ¬

```python
# ç”Ÿæˆéšæœºip
def random_ip():
    ip = ".".join(str(random.randint(0, 255)) for _ in range(4))
    return ip
ip = random_ip()

# æ‰‹åŠ¨æ·»åŠ çš„æŠ¥å¤´ï¼Œç”¨äºè§„é¿è±†ç“£åçˆ¬(è®¿é—®ç½‘é¡µè¿”å›çŠ¶æ€ç 418)
headers = {
    'User-Agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'X-Forwarded-For':
    ip,
    'Cookie':
    'bid=HNf-ab2-lJI; gr_user_id=031667b7-5f8a-4ede-b695-e52d9181fe11; __gads=ID=60db1851df53b133:T=1583253085:S=ALNI_MbBwCgmPG1hMoA4-Z0HSw_zcT0a0A; _vwo_uuid_v2=DD32BB6F8D421706DCD1CDE1061FB7A45|ef7ec70304dd2cf132f1c42b3f0610e7; viewed="1200840_27077140_26943161_25779298"; ll="118165"; __yadk_uid=5OpXTOy4NkvMrHZo7Rq6P8VuWvCSmoEe; ct=y; ap_v=0,6.0; push_doumail_num=0; push_noty_num=0; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1583508410%2C%22https%3A%2F%2Fwww.pypypy.cn%2F%22%5D; _pk_ses.100001.4cf6=*; dbcl2="131182631:x7xeSw+G5a8"; ck=9iia; _pk_id.100001.4cf6=17997f85dc72b3e8.1583492846.4.1583508446.1583505298.',
}

# çˆ¬å–ä¼‘æ¯
if __name__ == "__main__":
    ...
    delay = random.randint(0, 5)  # éšæœºé—´éš”0-5sè®¿é—®
    time.sleep(delay)
```

##### 2.å†…å®¹è§£æ

åœ¨part1/web_scraper.pyæ–‡ä»¶ä¸­ä½¿ç”¨BeautifulSoupè§£æé¡µé¢å†…å®¹ï¼Œä¹‹åæå–é¡µé¢ä¸­çš„å„ä¸ªä¿¡æ¯ã€‚ç”±äºé¡µé¢ä¸­çš„ä¿¡æ¯å¤§éƒ¨åˆ†æ˜¯è¿ç»­çš„textæ–‡æœ¬ï¼Œå¾ˆå¤šä¿¡æ¯ä¼šæ··åœ¨ä¸€èµ·ï¼Œé€šè¿‡å­—ç¬¦ä¸²åˆ‡å‰²æ¥è·å–éœ€è¦çš„ä¿¡æ¯ã€‚åŒæ—¶å› ä¸ºç½‘é¡µæ ¼å¼ä¸åŒï¼Œæœ‰çš„ç”µå½±/ä¹¦ç±å­˜åœ¨éƒ¨åˆ†æ•°æ®ç¼ºå¤±çš„æƒ…å†µï¼Œéœ€è¦æ·»åŠ åˆ¤æ–­è¯­å¥é¿å…ç¨‹åºæŠ¥é”™ã€‚

ä»¥è§£æç”µå½±å†…å®¹ä¸ºä¾‹ï¼š

```python
def search_douban_movie(id):
    ...
    # ä½¿ç”¨BeautifulSoupè§£æé¡µé¢å†…å®¹
    soup = BeautifulSoup(response.text, "html.parser")

    # æå–æœç´¢ç»“æœä¸­çš„ç”µå½±ä¿¡æ¯
    # è·å–ä¸­è‹±æ–‡åç§°
    temp = soup.find('span', {'property': 'v:itemreviewed'})
    if (temp is not None):
        name = temp.text
        # æˆªå–ä¸­æ–‡å
        chinese_name = name.split()[0]
        print('ä¸­æ–‡åç§°:', chinese_name)
        # æˆªå–å…¶ä»–åç§°
        if (name.split()[1:] != []):
            other_name = ' '.join(name.split()[1:])
            # print('å…¶ä»–åç§°:', other_name)
        else:
            other_name = None
    else:
        print("ç©ºé¡µé¢\n")
        return
  
    # è·å–å¹´ä»½
    temp = soup.find('span', class_='year')
    if (temp is not None):
        year = temp.text.strip()[1:5]
        # print('å¹´ä»½:', year)
    else:
        year = None

    # è·å–è¯„åˆ†
    temp = soup.find('strong', {'property': "v:average"})
    if (temp is not None):
        rate = temp.text
        # print('è¯„åˆ†:', rate)
    else:
        rate = None
  
    # è·å–å…¶ä»–ä¿¡æ¯
    info = soup.find('div', {'id': 'info'}).text

    # print(info[1:], '\n')
    index_1 = info.find('å¯¼æ¼”: ')
    index_2 = info.find('ç¼–å‰§: ')
    index_3 = info.find('ä¸»æ¼”: ')
    index_4 = info.find('ç±»å‹: ')
    index_5 = info.find('åˆ¶ç‰‡å›½å®¶/åœ°åŒº: ')
    index_6 = info.find('è¯­è¨€: ')
    index_7 = info.find('\n', index_6)
    if (index_1 != -1 and index_2 != -1):
        diretor = info[index_1 + len('å¯¼æ¼”: '):index_2 - 1]
        # print('å¯¼æ¼”:', diretor)
    else:
        diretor = None
    if (index_2 != -1 and index_3 != -1):
        scriptwriter = info[index_2 + len('ç¼–å‰§: '):index_3 - 1]
        # print('ç¼–å‰§:', scriptwriter)
    else:
        scriptwriter = None
    if (index_3 != -1 and index_4 != -1):
        actor = info[index_3 + len('ä¸»æ¼”: '):index_4 - 1]
        # print('ä¸»æ¼”:', actor)
    else:
        actor = None
    if (index_4 != -1 and index_5 != -1):
        type = info[index_4 + len('ç±»å‹: '):index_5 - 1]
        # print('ç±»å‹:', type)
    else:
        type = None
    if (index_5 != -1 and index_6 != -1):
        area = info[index_5 + len('åˆ¶ç‰‡å›½å®¶/åœ°åŒº: '):index_6 - 1]
        # print('åˆ¶ç‰‡å›½å®¶/åœ°åŒº:', area)
    else:
        area = None
    if (index_6 != -1 and index_7 != -1):
        language = info[index_6 + len('è¯­è¨€: '):index_7]
        # print('è¯­è¨€:', language)
    else:
        language = None
    detail = soup.find('span', {'class': 'all hidden'})  # å±•å¼€ç®€ä»‹
    if (detail is None):  # ä¸éœ€è¦å±•å¼€ç®€ä»‹
        detail = soup.find('span', {'property': 'v:summary'})
    detail = detail.text.strip().replace(' ', '')
    detail = detail.replace('\n\u3000\u3000', '')
    # print('ç®€ä»‹:\n', detail)
    print("=" * 40)

```

##### 3.å†…å®¹å†™å…¥

åœ¨part1/web_scraper.pyæ–‡ä»¶ä¸­ä½¿ç”¨pandasåº“ä¸­çš„æ•°æ®ç»“æ„DataFrameï¼Œå…ˆå°†çˆ¬å–çš„æ•°æ®è½¬åŒ–ä¸ºDataFrameç±»å‹ï¼Œç„¶åå°†æ•°æ®å†™å…¥åˆ°excelæ–‡æ¡£ã€‚

ä»¥ç”µå½±ä¸ºä¾‹ï¼š

```py
def search_douban_movie(id):
    ...
    global movie_list
    # ä¿¡æ¯å­—å…¸
    movie_list.append({
        "id": id,
        "cname": chinese_name,
        "ename": other_name,
        "year": year,
        "rate": rate,
        "director": diretor,
        "scr": scriptwriter,
        "star": actor,
        "type": type,
        "area": area,
        "lang": language,
        "syno": detail
    })
def movie_toExcel(data, fileName):  # pandasåº“å‚¨å­˜æ•°æ®åˆ°excel
    ids = []
    cnames = []
    enames = []
    years = []
    rates = []
    directors = []
    scrs = []
    stars = []
    types = []
    areas = []
    langs = []
    synos = []

    for i in range(len(data)):
        ids.append(data[i]["id"])
        cnames.append(data[i]["cname"])
        enames.append(data[i]["ename"])
        years.append(data[i]["year"])
        rates.append(data[i]["rate"])
        directors.append(data[i]["director"])
        scrs.append(data[i]["scr"])
        stars.append(data[i]["star"])
        types.append(data[i]["type"])
        areas.append(data[i]["area"])
        langs.append(data[i]["lang"])
        synos.append(data[i]["syno"])

    dfData = {  # ç”¨å­—å…¸è®¾ç½®DataFrameæ‰€éœ€æ•°æ®
        'åºå·': ids,
        'ä¸­æ–‡åç§°': cnames,
        'è‹±æ–‡åç§°': enames,
        'å¹´ä»½': years,
        'è¯„åˆ†': rates,
        'å¯¼æ¼”': directors,
        'ç¼–å‰§': scrs,
        'ä¸»æ¼”': stars,
        'ç±»å‹': types,
        'åˆ¶ç‰‡å›½å®¶/åœ°åŒº': areas,
        'è¯­è¨€': langs,
        'ç®€ä»‹': synos
    }

    df = pd.DataFrame(dfData)  # åˆ›å»ºDataFrame
    df.to_excel(fileName,
                index=False,
                sheet_name="{0}-{1}".format(LIST_POSITION, LIST_POSITION +
                                            LIST_SIZE))
```

å…·ä½“å†™å…¥æ—¶ï¼Œä¸ºäº†åˆ†æ®µå†™å…¥excelè¡¨æ ¼ï¼Œåšäº†ä»¥ä¸‹å¤„ç†ï¼š

```python
LIST_POSITION = 0 # è¯»å–çš„èµ·å§‹è¡Œæ•°
LIST_SIZE = 100 # ä¸€æ¬¡è¯»å–çš„è¡Œæ•°

book_list = []  # å­—å…¸åˆ—è¡¨
    count = 0
    times = 0
    for id in book_id_data:
        if (count < LIST_POSITION):
            count = count + 1
            continue
        if (times >= LIST_SIZE):
            break
        times = times + 1
        search_douban_book(id)
        book_toExcel(book_list, 'data/book.xlsx')
        delay = random.randint(0, 5)  # éšæœºé—´éš”0-5sè®¿é—®
        time.sleep(delay)
```

##### 4.å®éªŒç»“æœ

éƒ¨åˆ†å®éªŒç»“æœå¦‚ä¸‹ï¼Œå…¨éƒ¨ç»“æœè§æ–‡ä»¶part1/data/movie.xlsxå’Œæ–‡ä»¶part1/data/book.xlsxï¼ˆéƒ¨åˆ†é¡µé¢ä¸ºç©ºé¡µé¢ï¼Œç›´æ¥è·³è¿‡ï¼Œæ•…çˆ¬å–æ€»æ•°ä¸è¶³1200ï¼‰ã€‚

![image-20231030171146053](.\figs\image-20231030171146053.png)

![image-20231030171208270](.\figs\image-20231030171208270.png)

### äºŒ.æ£€ç´¢

#### å®éªŒè¦æ±‚ï¼š

1.å¯¹ä¸€é˜¶æ®µä¸­çˆ¬å–çš„ç”µå½±å’Œä¹¦ç±æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå°†æ–‡æœ¬è¡¨å¾ä¸ºå…³é”®è¯é›†åˆã€‚

2.åœ¨ç»è¿‡é¢„å¤„ç†çš„æ•°æ®é›†ä¸Šå»ºç«‹å€’æ’ç´¢å¼•è¡¨ğ‘ºï¼Œå¹¶ä»¥åˆé€‚çš„æ–¹å¼å­˜å‚¨ç”Ÿæˆçš„å€’æ’ç´¢å¼•æ–‡ä»¶ã€‚

3.å¯¹äºç»™å®šçš„ bool æŸ¥è¯¢ğ‘¸ğ’ƒğ’ğ’ğ’ï¼ˆä¾‹å¦‚ åŠ¨ä½œ and å‰§æƒ…ï¼‰ï¼Œæ ¹æ®ä½ ç”Ÿæˆçš„å€’æ’ç´¢å¼•è¡¨ ğ‘ºï¼Œè¿”å›ç¬¦åˆæŸ¥è¯¢è§„åˆ™ğ‘¸ğ’ƒğ’ğ’ğ’çš„ç”µå½±æˆ–/å’Œä¹¦ç±é›†åˆğ‘¨ğ’ƒğ’ğ’ğ’ = {ğ‘¨ğ’ƒğ’ğ’ğ’, ğ‘¨ğ’ƒğ’ğ’ğ’, â€¦ }ï¼Œå¹¶ä»¥åˆé€‚çš„æ–¹å¼å±•ç°ç»™ç”¨æˆ·ï¼ˆä¾‹å¦‚ç»™å‡ºç”µå½±åç§°å’Œåˆ†ç±»æˆ–æ˜¾ç¤ºéƒ¨åˆ†ç®€ä»‹ç­‰ï¼‰ã€‚

4.ä»»é€‰ä¸€ç§è¯¾ç¨‹ä¸­ä»‹ç»è¿‡çš„ç´¢å¼•å‹ç¼©åŠ ä»¥å®ç°ï¼Œå¦‚æŒ‰å—å­˜å‚¨ã€å‰ç«¯ç¼–ç ç­‰ï¼Œå¹¶æ¯”è¾ƒå‹ç¼©åçš„ç´¢å¼•åœ¨å­˜å‚¨ç©ºé—´å’Œæ£€ç´¢æ•ˆç‡ä¸Šä¸åŸç´¢å¼•çš„åŒºåˆ«ã€‚

#### å®éªŒæ­¥éª¤ï¼š

##### 1.åˆ†è¯

1ã€jiebaåˆ†è¯

jiebaåº“æ˜¯ç›®å‰åšçš„æœ€å¥½çš„pythonåˆ†è¯ç»„ä»¶ã€‚é¦–å…ˆå®ƒçš„å®‰è£…ååˆ†ä¾¿æ·ï¼Œåªéœ€è¦ä½¿ç”¨pipå®‰è£…ï¼›å…¶æ¬¡ï¼Œå®ƒä¸éœ€è¦å¦å¤–ä¸‹è½½å…¶å®ƒçš„æ•°æ®åŒ…ï¼Œåœ¨è¿™ä¸€ç‚¹ä¸Šå®ƒæ¯”å…¶ä½™äº”æ¬¾åˆ†è¯å·¥å…·éƒ½è¦ä¾¿æ·ã€‚å¦å¤–ï¼Œjiebaåº“æ”¯æŒçš„æ–‡æœ¬ç¼–ç æ–¹å¼ä¸ºutf-8ã€‚

Jiebaåº“åŒ…å«è®¸å¤šåŠŸèƒ½ï¼Œå¦‚åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€è‡ªå®šä¹‰è¯å…¸ã€å…³é”®è¯æå–ã€‚åŸºäºjiebaçš„å…³é”®è¯æå–æœ‰ä¸¤ç§å¸¸ç”¨ç®—æ³•ï¼Œä¸€æ˜¯TF-IDFç®—æ³•ï¼›äºŒæ˜¯TextRankç®—æ³•ã€‚åŸºäºjiebaåº“çš„åˆ†è¯ï¼ŒåŒ…å«ä¸‰ç§åˆ†è¯æ¨¡å¼ï¼š

- **ç²¾å‡†æ¨¡å¼**ï¼šè¯•å›¾å°†å¥å­æœ€ç²¾ç¡®åœ°åˆ‡å¼€ï¼Œé€‚åˆæ–‡æœ¬åˆ†æï¼‰ï¼›
- **å…¨æ¨¡å¼**ï¼šæŠŠå¥å­ä¸­æ‰€æœ‰çš„å¯ä»¥æˆè¯çš„è¯è¯­éƒ½æ‰«æå‡ºæ¥, é€Ÿåº¦éå¸¸å¿«ï¼Œä½†æ˜¯ä¸èƒ½è§£å†³æ­§ä¹‰ï¼‰ï¼›
- **æœç´¢å¼•æ“æ¨¡å¼**ï¼šæœç´¢å¼•æ“æ¨¡å¼ï¼Œåœ¨ç²¾ç¡®æ¨¡å¼çš„åŸºç¡€ä¸Šï¼Œå¯¹é•¿è¯å†æ¬¡åˆ‡åˆ†ï¼Œæé«˜å¬å›ç‡ï¼Œé€‚åˆç”¨äºæœç´¢å¼•æ“åˆ†è¯ã€‚

```python
def word_cut0(mytext):
    #jieba.load_userdict('exception.txt')  
    jieba.initialize()  # åˆå§‹åŒ–jieba
  
    # æ–‡æœ¬é¢„å¤„ç† ï¼šå»é™¤å­—ç¬¦å‰©ä¸‹ä¸­æ–‡
    new_data = re.findall('[\u4e00-\u9fa5]+', mytext, re.S)
    new_data = " ".join(new_data)
  
    # åˆ†è¯
    seg_list_exact = jieba.lcut(new_data)
    result_list = []
  
	â€¦â€¦ 
        
  
    return new_list
```

2ã€SnowNLP

æ˜¯ä¸€ä¸ªpythonå†™çš„ç±»åº“ï¼Œå¯ä»¥æ–¹ä¾¿çš„å¤„ç†ä¸­æ–‡æ–‡æœ¬å†…å®¹ï¼Œæ˜¯å—åˆ°äº†TextBlobçš„å¯å‘è€Œå†™çš„ï¼Œç”±äºç°åœ¨å¤§éƒ¨åˆ†çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“åŸºæœ¬éƒ½æ˜¯é’ˆå¯¹è‹±æ–‡çš„ï¼Œäºæ˜¯å†™äº†ä¸€ä¸ªæ–¹ä¾¿å¤„ç†ä¸­æ–‡çš„ç±»åº“ï¼Œå¹¶ä¸”å’ŒTextBlobä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œæ²¡æœ‰ç”¨NLTKï¼Œæ‰€æœ‰çš„ç®—æ³•éƒ½æ˜¯è‡ªå·±å®ç°çš„ï¼Œå¹¶ä¸”è‡ªå¸¦äº†ä¸€äº›è®­ç»ƒå¥½çš„å­—å…¸ã€‚æ³¨æ„æœ¬ç¨‹åºéƒ½æ˜¯å¤„ç†çš„unicodeç¼–ç ï¼Œæ‰€ä»¥ä½¿ç”¨æ—¶è¯·è‡ªè¡Œdecodeæˆunicodeã€‚ç®€å•æ¥è¯´ï¼Œsnownlpæ˜¯ä¸€ä¸ªä¸­æ–‡çš„è‡ªç„¶è¯­è¨€å¤„ç†çš„Pythonåº“ï¼Œsnownlpä¸»è¦åŠŸèƒ½æœ‰ï¼šä¸­æ–‡åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»ã€è½¬æ¢æˆæ‹¼éŸ³ã€ç¹ä½“è½¬ç®€ä½“ã€æå–æ–‡æœ¬å…³é”®è¯ã€æå–æ–‡æœ¬æ‘˜è¦ã€tfï¼Œidfã€Tokenizationã€æ–‡æœ¬ç›¸ä¼¼ã€‚

æ€»ç»“ï¼šsnowNLPåº“çš„æƒ…æ„Ÿåˆ†ææ¨¡å—ï¼Œä½¿ç”¨éå¸¸æ–¹ä¾¿ï¼ŒåŠŸèƒ½ä¹Ÿå¾ˆä¸°å¯Œã€‚

```python
def word_cut1(mytext):
    #s = SnowNLP(u'SnowNLPç±»ä¼¼NLTKï¼Œæ˜¯é’ˆå¯¹ä¸­æ–‡å¤„ç†çš„ä¸€ä¸ªPythonå·¥å…·åº“ã€‚')
    #words = s.words
    #print(words)
  
    # æ–‡æœ¬é¢„å¤„ç† ï¼šå»é™¤å­—ç¬¦å‰©ä¸‹ä¸­æ–‡
    new_data = re.findall('[\u4e00-\u9fa5]+', mytext, re.S)
    new_data = " ".join(new_data)
  
    # åˆ†è¯
    s = SnowNLP(new_data)
    words = s.words
    result_list = []
  
	â€¦â€¦ 
          
    return new_list
```

3ã€æ•ˆæœå¯¹æ¯”

![image-20231029204151474](.\figs\image-20231029204151474.png)

![image-20231029204246708](.\figs\image-20231029204246708.png)

![image-20231029205108763](.\figs\image-20231029205108763.png)å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªåˆ†è¯å·¥å…·æ•ˆæœå·®å¼‚ä¸å¤§ã€‚ä½†æ˜¯å¯¹äºç‰¹å®šæ„æ€çš„é•¿è¯æ±‡ï¼Œæ¯”å¦‚æ··è¡€ç‹å­ã€ç¬¬å…­éƒ¨ã€ç‹¼å›¾è…¾ã€ä¸­åæ–‡æ˜ã€é¾™çš„ä¼ äººã€å·´é»åœ£æ¯é™¢ï¼Œsnownlpå·¥å…·å°±ä¼šæŠŠå®ƒä»¬æ‹†å¼€ï¼Œæ‰€ä»¥æœ€ç»ˆé€‰ç”¨jiebaå·¥å…·ã€‚

æ¯”è¾ƒç»“æœè¾“å‡ºåœ¨part1/data/book_com.xlsxã€‚

4ã€å»åœç”¨è¯

```python
    # åŠ è½½åœç”¨è¯è¡¨
    with open('stop_words.txt', encoding='utf-8') as f:
        con = f.readlines()
        stop_words = set()
        for i in con:
            i = i.replace("\n", "")   # å»æ‰æ¯ä¸€è¡Œçš„\n
            stop_words.add(i)

    # å»é™¤åœç”¨è¯ å»é™¤å•å­—
    for word in seg_list_exact:
        if word not in stop_words and len(word) > 1:
            result_list.append(word) 
```

5ã€å»åŒä¹‰è¯

```python
def tyc(string1):
    # txtæ˜¯åŒä¹‰è¯è¡¨ï¼Œæ¯è¡Œæ˜¯ä¸€ç³»åˆ—åŒä¹‰è¯ï¼Œç”¨ åˆ†å‰²
    # è¯»å–åŒä¹‰è¯è¡¨ï¼šå¹¶ç”Ÿæˆä¸€ä¸ªå­—å…¸ã€‚
    combine_dict = {}
    for line in open("jyc.txt", "r", encoding='utf-8'):
        #print(line)
        seperate_word = line.strip().split(" ")
        #print(seperate_word)
        num = len(seperate_word)
        #print(num)
        for i in range(1, num):
            combine_dict[seperate_word[i]] = seperate_word[0]
          

    # å°†åˆ†è¯ä»¥/ä¸ºé—´éš”åˆå¹¶
    #seg_list = jieba.lcut(string1, cut_all = False)
    #print(seg_list)
    #f = "/".join(string1)  
    #print (f)

    str0 = []
    strlen = len(string1)
    for j in range(0, strlen):
        if string1[j] in combine_dict:
            str0.append(combine_dict[string1[j]])
        else:
            str0.append(string1[j])

    # print final_sentence
    return str0
```

ä¸»å‡½æ•°ä¸­è°ƒç”¨ï¼š

```python
	new_list = list(set(tyc(result_list)))
    new_list.sort(key = tyc(result_list).index)	#  ä¿æŒåŸæ¥é¡ºåº
```



6.æ·»åŠ Tag

åœ¨å¯¹ç”µå½±ç®€ä»‹/ä¹¦ç±ç®€ä»‹åˆ†è¯ç»“æŸå†™å…¥è¡¨æ ¼åï¼Œpart1/add_tag_to_keyå°†æä¾›çš„Tagä¹ŸåŠ å…¥åˆ°å…³é”®è¯ã€‚ä¸‹é¢ä»¥æ·»åŠ ç”µå½±Tagä¸ºä¾‹ï¼š

```python
# è·å–tagä¿¡æ¯ï¼Œå¹¶åŠ å…¥å…³é”®è¯
import pandas as pd
# è¯»å–æ–‡ä»¶å­˜å…¥åˆ—è¡¨å˜é‡
df1 = pd.read_csv('part1/data/Movie_tag.csv').fillna('')
movie_id_1 = df1['Id'].tolist()
movie_tag = df1['Tag'].tolist()
df2 = pd.read_excel('part1/data/movie.xlsx').fillna('')
movie_id_2 = df2['åºå·'].tolist()
# movie_type = df2['ç±»å‹'].tolist()
movie_id_key = df2['å…³é”®è¯'].tolist()
# å¾ªç¯æŸ¥æ‰¾éœ€è¦ä¿®æ”¹çš„åœ°æ–¹
for i in range(len(movie_id_2)):
    movie_id_key[i] = movie_id_key[i].replace(' ', '')

    # å¯¹æŒ‡å®šå•å…ƒæ ¼ä¿®æ”¹,æ–¹æ‹¬å·ä¸­çš„å‚æ•°ç¬¬ä¸€ä¸ªæ˜¯è¡Œå·ï¼Œç¬¬äºŒä¸ªæ˜¯åˆ—å
    df2.at[i, 'å…³é”®è¯'] = movie_id_key[i]
    # print(movie_id_key[index])
# å†™å›æ–‡ä»¶ï¼Œä¸€å®šä¸èƒ½å¿˜äº†è¿™ä¸€æ­¥
df2.to_excel('part1/data/movie.xlsx', index=False)
```

##### 2.å»ºç«‹å€’æ’è¡¨

åœ¨part1/in verted_index_to_excelä¸­ï¼Œå¯¹ç…§è·å–åˆ°çš„å…³é”®è¯ä¿¡æ¯ï¼Œé€šè¿‡pandasè¯»å–IDå’Œå¯¹åº”çš„å…³é”®è¯ï¼Œæ„å»ºå…³é”®è¯-IDå­—å…¸ï¼Œå†å°†å­—å…¸å†™å…¥excelè¡¨æ ¼ã€‚ä¸ºäº†æ–¹ä¾¿åç»­çš„Boolæ£€ç´¢ï¼ŒIDé‡‡ç”¨é›†åˆæ•°æ®ç»“æ„ã€‚ä»¥ç”µå½±ä¸ºä¾‹ï¼š

```python
import pandas as pd

df1 = pd.read_excel('part1/data/movie.xlsx').fillna('')
ids = df1['åºå·'].tolist()
keys = df1['å…³é”®è¯'].tolist()
Inverted_Index = dict()
id_list = []


# åˆ›å»ºå€’æ’è¡¨
def create_inverted_index():
    for i in range(len(keys)):
        keywords = keys[i].split(',')  # å°†æ¯ä¸ªidçš„å…³é”®è¯ç”¨é€—å·åˆ‡å‰²ï¼Œå˜æˆä¸€ä¸ªåˆ—è¡¨å˜é‡
        for item in keywords:  # éå†idçš„æ¯ä¸ªå…³é”®è¯
            if item not in Inverted_Index:
                Inverted_Index[item] = set()  # ä¸åœ¨å€’æ’è¡¨ä¸­çš„å…³é”®è¯æ–°å»ºé›†åˆ
            Inverted_Index[item].add(ids[i])  # åœ¨å€’æ’è¡¨å¯¹åº”çš„å…³é”®è¯ä½ç½®æ·»åŠ id
    for item in Inverted_Index:
        id_list.append({'key': item, 'id': Inverted_Index[item]})
    print(Inverted_Index)


def list_to_excel():
    keys = [item['key'] for item in id_list]
    ids = [item['id'] for item in id_list]
    dfData = {'å…³é”®è¯': keys, 'ID': ids}
    df = pd.DataFrame(dfData)  # åˆ›å»º DataFrame
    df.to_excel('part1/data/movie_list.xlsx', index=False)


create_inverted_index()
list_to_excel()
```

å€’æ’è¡¨ç»“æœè§part1/data/movie_listå’Œpart1/data/book_list

![image-20231030170747442](.\figs\image-20231030170747442.png)

![image-20231030171755887](.\figs\image-20231030171755887.png)

##### 3.å¸ƒå°”æŸ¥è¯¢

åœ¨part1/keyword_searchä¸­å®ç°äº†boolæ£€ç´¢ã€‚

é¦–å…ˆæ˜¯æ ¹æ®å¯¹æ£€ç´¢è¯­å¥è¿›è¡Œä¼˜å…ˆçº§ï¼Œå…³é”®è¯çš„åˆ†æã€‚æŒ‰ç…§boolæ£€ç´¢çš„è¿ç®—ç¬¦ä¼˜å…ˆçº§ï¼šæ‹¬å·>NOT>AND=ORï¼Œæ„å»ºå…³é”®è¯æ ˆå’Œè¿ç®—ç¬¦æ ˆï¼Œå…ˆè¿›è¡Œé«˜ä¼˜å…ˆçº§è¿ç®—ç¬¦çš„è®¡ç®—ã€‚åœ¨è¿™é‡Œä¸è€ƒè™‘ANDå’ŒORçš„ä¼˜å…ˆçº§å·®å¼‚ï¼Œé»˜è®¤è¾“å…¥ä¼šé€šè¿‡æ‹¬å·æ¥è§„é¿ä¼˜å…ˆçº§é—®é¢˜ã€‚å› ä¸ºä¸Šä¸€æ­¥é‡‡ç”¨äº†é›†åˆçš„æ•°æ®ç»“æ„å­˜å‚¨å€’æ’è¡¨ä¸­çš„IDï¼Œè¿™ä¸€æ­¥å¯¹è¿ç®—ç¬¦çš„å¤„ç†å°±å¯ä»¥ç›´æ¥è°ƒç”¨é›†åˆè¿ç®—äº†ã€‚

è€Œå¯¹äºå…³é”®è¯åœ¨å€’æ’è¡¨ä¸­çš„æŸ¥æ‰¾ï¼Œé‡‡ç”¨ç®€å•çš„åˆ—è¡¨æŸ¥è¯¢å°±å¥½ã€‚

```python
def search_in_movielist(search_input):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥åˆ†å‰²æŸ¥è¯¢å­—ç¬¦ä¸²
    tokens = re.split(r'(\(|\)|AND|OR|NOT)', search_input)

    # å®šä¹‰å¸ƒå°”æ“ä½œçš„å‡½æ•°
    def intersection(posting1, posting2):
        return set(posting1) & set(posting2)

    def union(posting1, posting2):
        return set(posting1) | set(posting2)

    def negation(posting):
        all_docs = set(movie_id)
        return all_docs - set(posting)

    stack = []
    operator_stack = []

    for token in tokens:
        token = token.strip()
        if token.strip() == "":
            continue
        if token == "(":
            operator_stack.append(token)
        elif token == ")":
            while operator_stack and operator_stack[-1] != "(":
                operator = operator_stack.pop()
                if operator == 'NOT':
                    operand = stack.pop()
                    stack.append(negation(operand))
                elif operator == "AND":
                    operand2 = stack.pop()
                    operand1 = stack.pop()
                    stack.append(intersection(operand1, operand2))
                elif operator == "OR":
                    operand2 = stack.pop()
                    operand1 = stack.pop()
                    stack.append(union(operand1, operand2))
        elif token == "NOT":
            operator_stack.append(token)
        elif token == "AND" or token == "OR":
            while operator_stack and operator_stack[-1] in (
                    "AND", "OR") and operator_stack[-1] != "(":
                operator = operator_stack.pop()
                operand2 = stack.pop()
                operand1 = stack.pop()
                if operator == "AND":
                    stack.append(intersection(operand1, operand2))
                elif operator == "OR":
                    stack.append(union(operand1, operand2))
            operator_stack.append(token)

        else:
            # å¤„ç†å…³é”®è¯
            if token in movie_list_key:
                index = movie_list_key.index(token)
                if (len(operator_stack) > 0 and operator_stack[-1] == 'NOT'):
                    operator_stack.pop()
                    stack.append(negation(eval(movie_list_id[index])))
                else:
                    stack.append(eval(movie_list_id[index]))
            else:
                stack.append(set())  # æœªçŸ¥è¯çš„ç»“æœä¸ºç©ºé›†

    while operator_stack:
        operator = operator_stack.pop()
        if operator == "AND" or operator == "OR":
            operand2 = stack.pop()
            operand1 = stack.pop()
            if operator == "AND":
                stack.append(intersection(operand1, operand2))
            elif operator == "OR":
                stack.append(union(operand1, operand2))
        elif operator == "NOT":
            operand = stack.pop()
            stack.append(negation(operand))
    # print(stack[0])
    print_movie_info(stack[0])
```

å½“æ‰¾åˆ°éœ€è¦çš„idåï¼Œè¿˜éœ€è¦ä»¥åˆé€‚çš„å½¢å¼å‘ˆç°æœç´¢ç»“æœ

```python
# æ‰“å°æœç´¢å¾—åˆ°çš„idsçš„ç›¸å…³ä¿¡æ¯
def print_movie_info(ids):
    for id in ids:
        index = movie_id.index(id)  # æ‰¾åˆ°idçš„index
        print('ID:', id)
        print('ç”µå½±åç§°:', movie_name[index])
        print('ç”µå½±è¯„åˆ†:', movie_rate[index])
        print('ç”µå½±ç±»å‹:', movie_type[index])
        print('ç”µå½±ç®€ä»‹:', movie_info[index])
        print(40 * "=")
```

ä¸‹é¢æ˜¯å¯¹å­¦å·å¯¹åº”çš„ç”µå½±çš„æ£€ç´¢ç»“æœï¼š

åˆ†åˆ«é€‰å–æ’åä¸º9,22,27çš„ä¸‰éƒ¨ç”µå½±

![image-20231030194225907](.\figs\image-20231030194225907.png)

![image-20231030195647971](.\figs\image-20231030195647971.png)

![image-20231030195705833](.\figs\image-20231030195705833.png)

![image-20231030202531376](.\figs\image-20231030202531376.png)

![image-20231030193300418](.\figs\image-20231030193300418.png)

![image-20231030193953368](.\figs\image-20231030193953368.png)

##### 4.å¼•ç´¢å‹ç¼©

å› ä¸ºæ¶‰åŠåˆ°çš„å…³é”®è¯å¤šå¤§25000ä¸ªï¼Œè¿™ä¸ªè§„æ¨¡çš„å€’æ’è¡¨å¦‚æœæ”¾åœ¨å†…å­˜é‡Œå¾ˆå ç”¨ç©ºé—´ï¼Œä¸”ä¸åˆ©äºæ£€ç´¢ã€‚part1/inverted_index_zipé€šè¿‡åˆ†å—å­˜å‚¨å¯¹å€’æ’è¡¨è¿›è¡Œäº†å‹ç¼©ã€‚

åœ¨è¿™é‡Œå°†100ä¸ªå…³é”®è¯ä½œä¸ºä¸€ç»„ã€‚å½“ç»„æ»¡æ—¶ï¼Œç”Ÿæˆä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶å­˜å‚¨å‹ç¼©çš„å€’æ’è¡¨ã€‚æœ€åç”Ÿæˆä¸€ä¸ªå…ƒæ•°æ®æŸ¥è¯¢è¡¨ï¼Œå°†å—å·å’Œå—å†…å­˜å‚¨çš„å…³é”®è¯è®°å½•åœ¨å…ƒæ•°æ®æŸ¥è¯¢è¡¨ä¸­ã€‚

```python
# æ¯100ä¸ªå…³é”®è¯ä¸ºä¸€ç»„ï¼Œæ„å»ºå‹ç¼©ç´¢å¼•è¡¨
def zip_inverted_index():
    # if os.path.exists('part1/data/block_metadata.pkl'):  # å­˜åœ¨å‹ç¼©çš„ç´¢å¼•è¡¨åï¼Œä¸å†æ„å»ºå‹ç¼©ç´¢å¼•è¡¨
    #     return
    df_movie_list = pd.read_excel('part1/data/movie_list.xlsx').fillna('')
    movie_list_key = df_movie_list['å…³é”®è¯'].tolist()
    movie_list_id = df_movie_list['ID'].tolist()
    block_size = 100  # å—çš„å¤§å°
    block_number = 0  # å—çš„ç¼–å·
    current_block = {}  # å½“å‰å—çš„å€’æ’ç´¢å¼•
    block_metadata = []  # å­˜å‚¨å—çš„å…ƒæ•°æ®ä¿¡æ¯
    for i in range(len(movie_list_id)):
        keyword = movie_list_key[i]
        current_block[keyword] = eval(movie_list_id[i])
        # current_block[keyword].add(eval(movie_list_id[i]))
        if len(current_block) >= block_size:
            block_number += 1
            block_filename = f"part1/block/block_{block_number}.pkl"  # æ–‡ä»¶åç¤ºä¾‹
            with open(block_filename, 'wb') as block_file:
                pickle.dump(current_block, block_file)
            block_metadata.append({
                'block_number': block_number,
                'keywords': list(current_block.keys())
            })
            current_block = {}
    for item in current_block:
        id_list.append({'key': item, 'id': current_block[item]})

    with open('part1/block_metadata.pkl', 'wb') as metadata_file:
        pickle.dump(block_metadata, metadata_file)

```

ä¸‹é¢éœ€è¦åˆ©ç”¨å‹ç¼©çš„å€’æ’è¡¨è¿›è¡Œæ£€ç´¢ã€‚å¯¹äºå…³é”®è¯keywordï¼Œéœ€è¦éå†å…ƒæ•°æ®æŸ¥è¯¢è¡¨ï¼Œæ‰¾åˆ°åæ‰“å¼€å¯¹åº”çš„å—ï¼Œå¹¶è¿”å›éœ€è¦çš„ID

```python
# å‹ç¼©ç´¢å¼•æ£€ç´¢
def search_in_zip(keyword):
    with open('part1/data/block_metadata.pkl', 'rb') as metadata_file:
        block_metadata = pickle.load(metadata_file)

    for block_info in block_metadata:
        block_filename = f"part1/block/block_{block_info['block_number']}.pkl"
        with open(block_filename, 'rb') as block_file:
            current_block = pickle.load(block_file)
            if keyword in current_block:
                # print(current_block[keyword])
                return current_block[keyword]
```

ä¸ºäº†ä½“ç°æ€§èƒ½å·®å¼‚ï¼Œåšå¦‚ä¸‹å¯¹æ¯”

```python
zip_inverted_index()
# æ£€ç´¢æ€§èƒ½è®¡æ—¶,ç´¢å¼•å‹ç¼©
start_time = time.time()
# æ¨¡æ‹Ÿæ£€ç´¢æ“ä½œ
keyword = "å®‰è¿ª"
search_in_zip(keyword)
keyword = "æ­¦å£«åˆ€"
search_in_zip(keyword)
keyword = "å¤§å±å¹•"
search_in_zip(keyword)
end_time = time.time()
elapsed_time = end_time - start_time
print(f"å‹ç¼©æ£€ç´¢ä»£ç è¿è¡Œæ—¶é—´: {elapsed_time} ç§’")

# æ£€ç´¢æ€§èƒ½è®¡æ—¶ï¼Œéå‹ç¼©

start_time = time.time()
# æ¨¡æ‹Ÿæ£€ç´¢æ“ä½œ
df_movie_list = pd.read_excel('part1/data/movie_list.xlsx').fillna('')
movie_list_key = df_movie_list['å…³é”®è¯'].tolist()
movie_list_id = df_movie_list['ID'].tolist()

keyword = "å®‰è¿ª"
search_in_list(keyword)
keyword = "æ­¦å£«åˆ€"
search_in_list(keyword)
keyword = "å¤§å±å¹•"
search_in_list(keyword)
end_time = time.time()
elapsed_time = end_time - start_time
print(f"é¡ºåºæ£€ç´¢ä»£ç è¿è¡Œæ—¶é—´: {elapsed_time} ç§’")
```

ç»“æœå¦‚ä¸‹ï¼š

![image-20231030174246161](.\figs\image-20231030174246161.png)

å¯¹äºåœ¨movieå€’æ’è¡¨ä¸­éšæœºé€‰å–çš„ä¸‰ä¸ªå…³é”®è¯ï¼Œé¡ºåºæ£€ç´¢èŠ±è´¹çš„æ—¶é—´æ˜¯è¿œè¿œå¤§äºå‹ç¼©æ£€ç´¢çš„ã€‚å½“æ£€ç´¢å…³é”®è¯å¢å¤šï¼Œè¿™ä¸ªå·®å¼‚ä¼šæ›´åŠ æ˜¾è‘—ã€‚è€Œä¸”å¦‚æœå°†å€’æ’è¡¨æ•´ä¸ªåŠ è½½åˆ°éå¸¸å ç”¨ç©ºé—´ã€‚è€Œå‹ç¼©ç´¢å¼•åï¼Œæ¯æ¬¡åªéœ€è¦æŠŠä¸€ä¸ªå—çš„æ•°æ®åŠ è½½åˆ°å†…å­˜ï¼Œé•¿æœŸå­˜åœ¨äºå†…å­˜ä¸­çš„åªæœ‰å…ƒæ•°æ®åˆ—è¡¨ï¼Œæå¤§åœ°èŠ‚çœäº†ç©ºé—´ã€‚

## ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨è±†ç“£æ•°æ®è¿›è¡Œæ¨è

**ï¼ˆ1ï¼‰æ•°æ®åˆ’åˆ†**

æ ¹æ®50%æä¾›å¯¹ç”¨æˆ·æ•°æ®åˆ’åˆ†ä»£ç ï¼Œå®é™…å®éªŒä¸­ç”¨äºé¢„æµ‹çš„æ•°æ®æŠ¹å»æ‰“åˆ†åˆ†å€¼ã€‚

```
# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_data, test_data = train_test_split(loaded_data,test_size=0.5,random_state=42)
```

**ï¼ˆ2ï¼‰è¯„åˆ†æ’åº**

å¯¹ä¸Šé¢æŠ¹å»åˆ†å€¼çš„å¯¹è±¡è¿›è¡Œé¡ºåºä½ç½®é¢„æµ‹ï¼Œå°†é¢„æµ‹å‡ºçš„å¯¹è±¡é¡ºåºä¸å®é™…çš„é¡ºåºè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ç”¨NDCGï¼ˆå…¨éƒ¨æ•°æ®æˆ–Topkï¼‰ è¯„ä¼°é¢„æµ‹æ•ˆæœã€‚



bert-base-chinese.py:ä½¿ç”¨BERTæ¨¡å‹å¯¹ä¸­æ–‡æ ‡ç­¾è¿›è¡Œç¼–ç å¹¶ç”Ÿæˆpklæ–‡ä»¶

bert-craft.py:è¯»å–å’Œå¤„ç†CSVæ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨BERTæ¨¡å‹å¯¹æ ‡ç­¾è¿›è¡Œç¼–ç ï¼Œæœ€ç»ˆå°†ç¼–ç åçš„æ ‡ç­¾å‘é‡ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ã€‚

embedding.py:è®­ç»ƒæ—¶ç”¨çš„ç±»å’Œæ•°æ®é›†æ¨¡å‹

train.py:æ ¹æ®æ˜ å°„è¡¨pklè®­ç»ƒï¼Œå¯¼å‡ºæœ€ç»ˆpoints.csv

points.csv:ç”¨æˆ·å¯¹ä½œå“çš„å®é™…æ‰“åˆ†å’Œé¢„æµ‹æ‰“åˆ†

ndcg.py:å¯¼å…¥points.csvï¼Œè®¡ç®—æ¯ä¸ªç”¨æˆ·å„è‡ªçš„ndcgå¹¶å¯¹å…¨ä½“ç”¨æˆ·çš„ndcgå–å¹³å‡



æ ¹æ®æä¾›çš„tagï¼Œå®éªŒç¬¬ä¸€é˜¶æ®µè·å¾—ç½‘é¡µä¿¡æ¯ç­‰ï¼Œæ·»åŠ æ–‡æœ¬ä¿¡æ¯è¿›è¡Œè¾…åŠ©é¢„æµ‹ ã€‚ä½¿ç”¨chinese-bertç­‰é¢„è®­ç»ƒæ¨¡å‹ï¼ŒæŠ½å–part1çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæ·»åŠ åˆ°embeddingä¸­æ¥è¡¥å…¨ä¿¡æ¯ã€‚

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese').cuda()

# è¯»å–å’Œå¤„ç†CSVæ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨BERTæ¨¡å‹å¯¹æ ‡ç­¾è¿›è¡Œç¼–ç ï¼Œæœ€ç»ˆå°†ç¼–ç åçš„æ ‡ç­¾å‘é‡ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ã€‚
# ä½¿ç”¨pandasåº“çš„`read_csv`å‡½æ•°ä»CSVæ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨`loaded_data`å˜é‡ä¸­ã€‚
loaded_data = pd.read_csv('part2/data/selected_book_top_1200_data_tag.csv') # part1æ•°æ®tag
more_tags = pd.read_excel('part1/data/book.xlsx').fillna('')
book_id = more_tags['åºå·'].tolist()
book_tag = more_tags['å…³é”®è¯'].tolist()

# åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨æ ‡ç­¾çš„å‘é‡è¡¨ç¤ºã€‚
tag_embedding_dict = {}
# åœ¨æ­¤ä»£ç å—ä¸­ï¼Œç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥æé«˜ä»£ç çš„è¿è¡Œæ•ˆç‡ã€‚
with torch.no_grad():
    # å¯¹`loaded_data`ä¸­çš„æ¯ä¸€è¡Œè¿›è¡Œè¿­ä»£ã€‚
    for index, rows in tqdm(loaded_data.iterrows()):
        # å°†æ ‡ç­¾åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        tags_str = " ".join(rows.Tags)
        # å°†ç¬¬ä¸€éƒ¨åˆ†æ‰¾åˆ°çš„å…³é”®è¯åŠ å…¥åˆ°tags_strä¸­
        book = rows.Book
        if (book in book_id):
            tags_str = tags_str[:-2]
            index = book_id.index(book)
            add_tags = book_tag[index].split(',')
            for item in add_tags:
                tags_str += f",   '{item}'"
            tags_str += ' }'
        # print(tags_str)
        # ä½¿ç”¨BERTæ¨¡å‹çš„tokenizerå¯¹æ ‡ç­¾æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚å‚æ•°truncation=TrueæŒ‡ç¤ºtokenizeråœ¨éœ€è¦æ—¶æˆªæ–­æ–‡æœ¬ï¼Œreturn_tensors='pt'è¡¨ç¤ºè¿”å›PyTorchå¼ é‡æ ¼å¼çš„ç¼–ç ç»“æœã€‚
        inputs = tokenizer(tags_str, truncation=True, return_tensors='pt')
        # print(inputs)
        # ä½¿ç”¨BERTæ¨¡å‹modelæ¥å¯¹è¾“å…¥çš„ç¼–ç ç»“æœè¿›è¡Œå¤„ç†ï¼Œé€šè¿‡inputs.input_idsã€inputs.token_type_idså’Œinputs.attention_maskä¼ å…¥æ¨¡å‹ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªå…³äºæ ‡ç­¾çš„åµŒå…¥è¡¨ç¤ºã€‚
        outputs = model(inputs.input_ids.cuda(), inputs.token_type_ids.cuda(),
                        inputs.attention_mask.cuda())
        # ä½¿ç”¨æœ€åä¸€å±‚çš„å¹³å‡éšè—çŠ¶æ€ä½œä¸ºæ ‡ç­¾çš„å‘é‡è¡¨ç¤º
        tag_embedding = outputs.last_hidden_state.mean(dim=1).cpu()
        tag_embedding_dict[rows.Book] = tag_embedding

import pickle

# å°†æ˜ å°„è¡¨å­˜å‚¨ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
with open('part2/data/tag_embedding_dict.pkl', 'wb') as f:
    pickle.dump(tag_embedding_dict, f)

# ä»äºŒè¿›åˆ¶æ–‡ä»¶ä¸­è¯»å–æ˜ å°„è¡¨
with open('part2/data/tag_embedding_dict.pkl', 'rb') as f:
    tag_embedding_dict = pickle.load(f)
    # print(tag_embedding_dict)

# è¯»loaded_dataå–ä¿å­˜çš„ CSV æ–‡ä»¶
loaded_data = pd.read_csv('part2\\data\\book_score.csv')

# æ˜¾ç¤ºåŠ è½½çš„æ•°æ®
print(loaded_data)device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese').cuda()

# è¯»å–å’Œå¤„ç†CSVæ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨BERTæ¨¡å‹å¯¹æ ‡ç­¾è¿›è¡Œç¼–ç ï¼Œæœ€ç»ˆå°†ç¼–ç åçš„æ ‡ç­¾å‘é‡ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ã€‚
# ä½¿ç”¨pandasåº“çš„`read_csv`å‡½æ•°ä»CSVæ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨`loaded_data`å˜é‡ä¸­ã€‚
loaded_data = pd.read_csv('part2\data\selected_book_top_1200_data_tag.csv')
# åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨æ ‡ç­¾çš„å‘é‡è¡¨ç¤ºã€‚
tag_embedding_dict = {}
# åœ¨æ­¤ä»£ç å—ä¸­ï¼Œç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥æé«˜ä»£ç çš„è¿è¡Œæ•ˆç‡ã€‚
with torch.no_grad():
    # å¯¹`loaded_data`ä¸­çš„æ¯ä¸€è¡Œè¿›è¡Œè¿­ä»£ã€‚
    for index, rows in tqdm(loaded_data.iterrows()):
        # å°†æ ‡ç­¾åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        tags_str = " ".join(rows.Tags)
        # print(tags_str)
        # ä½¿ç”¨BERTæ¨¡å‹çš„tokenizerå¯¹æ ‡ç­¾æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚å‚æ•°truncation=TrueæŒ‡ç¤ºtokenizeråœ¨éœ€è¦æ—¶æˆªæ–­æ–‡æœ¬ï¼Œreturn_tensors='pt'è¡¨ç¤ºè¿”å›PyTorchå¼ é‡æ ¼å¼çš„ç¼–ç ç»“æœã€‚
        inputs = tokenizer(tags_str, truncation=True, return_tensors='pt')
        # print(inputs)
        # ä½¿ç”¨BERTæ¨¡å‹modelæ¥å¯¹è¾“å…¥çš„ç¼–ç ç»“æœè¿›è¡Œå¤„ç†ï¼Œé€šè¿‡inputs.input_idsã€inputs.token_type_idså’Œinputs.attention_maskä¼ å…¥æ¨¡å‹ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªå…³äºæ ‡ç­¾çš„åµŒå…¥è¡¨ç¤ºã€‚
        outputs = model(inputs.input_ids.cuda(), inputs.token_type_ids.cuda(),
                        inputs.attention_mask.cuda())
        # ä½¿ç”¨æœ€åä¸€å±‚çš„å¹³å‡éšè—çŠ¶æ€ä½œä¸ºæ ‡ç­¾çš„å‘é‡è¡¨ç¤º
        tag_embedding = outputs.last_hidden_state.mean(dim=1).cpu()
        tag_embedding_dict[rows.Book] = tag_embedding
```

æ ·ä¾‹ä»£ç ä¸­ä»…ä»…åˆå¹¶tagæ¥èšåˆä¿¡æ¯ï¼Œå› æ­¤æ•ˆæœæœ‰è¾ƒå¤§æå‡ç©ºé—´ã€‚æ‰€ä»¥é‡‡ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ+è¾“å‡ºå±‚æ›¿ä»£æ ·ä¾‹ä»£ç ä¸­çš„ç®€å•å†…ç§¯æ“ä½œã€‚ self.mlpä¸ºå¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œï¼ŒåŠ å…¥éçº¿æ€§çš„reluä½œæ¿€æ´»å‡½æ•°ï¼Œä½¿ç”¨ä¸²è”æ“ä½œè·å¾—ç”¨æˆ·å’Œç‰©å“å‘é‡ã€‚

```python
class MatrixFactorization(nn.Module):
    def __init__(self, num_users, num_books, embedding_dim, hidden_dim):
        super(MatrixFactorization, self).__init__()
        self.user_embeddings = nn.Embedding(num_users, embedding_dim)
        self.book_embeddings = nn.Embedding(num_books, embedding_dim)
        self.linear_embedding = nn.Linear(hidden_dim, embedding_dim)
        # self.output = nn.Linear(embedding_dim, 6)
        self.mlp = nn.Sequential(
            nn.Linear(embedding_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, user, book, tag_embedding):
        user_embedding = self.user_embeddings(user)
        book_embedding = self.book_embeddings(book)
        tag_embedding_proj = self.linear_embedding(tag_embedding)
        book_intergrate = book_embedding + tag_embedding_proj
        # ä½¿ç”¨ä¸²è”æ“ä½œè·å¾—ç”¨æˆ·å’Œç‰©å“å‘é‡
        concatenated = torch.cat((user_embedding, book_intergrate), dim = 1)
        # å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œ
        predict = self.mlp(concatenated)
        return predict
```

ä½¿ç”¨ä¸Šè¿°æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯¼å‡ºç»“æœï¼Œä»¥ä¾›ä¸‹ä¸€é˜¶æ®µåˆ†æã€‚

```python
model = MatrixFactorization(num_users, num_books, embedding_dim,
                            hidden_state).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

num_epochs = 20
lambda_u, lambda_b = 0.001, 0.001

for epoch in range(num_epochs):
    model.train()
    total_loss_train, total_loss_test = 0.0, 0.0

    for idx, (user_ids, book_ids, ratings,
              tag_embedding) in tqdm(enumerate(train_dataloader)):
        # ä½¿ç”¨user_ids, book_ids, ratingsè¿›è¡Œè®­ç»ƒ

        optimizer.zero_grad()

        predictions = model(user_ids.to(device), book_ids.to(device),
                            tag_embedding.squeeze(1).to(device))
        loss = criterion(
            predictions,
            ratings.to(device)) + lambda_u * model.user_embeddings.weight.norm(
                2) + lambda_b * model.book_embeddings.weight.norm(2)

        loss.backward()
        optimizer.step()

        total_loss_train += loss.item()

        # if idx % 100 == 0:
        #     print(f'Step {idx}, Loss: {loss.item()}')

    output_loss_train = total_loss_train / (idx + 1)
    print(f'Epoch {epoch}, Train loss: {output_loss_train}')

    results = []
    model.eval()

    with torch.no_grad():
        for idx, (user_ids, item_ids, true_ratings,tag_embedding) in enumerate(test_dataloader):
            pred_ratings = model(user_ids.to(device), item_ids.to(device),tag_embedding.squeeze(1).to(device))

            loss = criterion(pred_ratings, ratings.to(device))
            total_loss_test += loss.item()
            
            # å°†ç»“æœè½¬æ¢ä¸º numpy arrays
            user_ids_np = user_ids.long().cpu().numpy().reshape(-1, 1)
            pred_ratings_np = pred_ratings.cpu().numpy().reshape(-1, 1)
            true_ratings_np = true_ratings.numpy().reshape(-1, 1)

            # å°†è¿™ä¸‰ä¸ª arrays åˆå¹¶æˆä¸€ä¸ª 2D array
            batch_results = np.column_stack(
                (user_ids_np, pred_ratings_np, true_ratings_np))

            # å°†è¿™ä¸ª 2D array æ·»åŠ åˆ° results
            results.append(batch_results)

        # å°†ç»“æœçš„ list è½¬æ¢ä¸ºä¸€ä¸ªå¤§çš„ numpy array
        results = np.vstack(results)

        print(f'Epoch {epoch}, Test loss:, {total_loss_test / (idx + 1)}')

        # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results, columns=['user', 'pred', 'true'])
        results_df['user'] = results_df['user'].astype(int)


outputpath='.\part2\points.csv'
results_df.to_csv(outputpath,sep=',',index=False,header=True)
```

è®¡ç®—NDCGè¯„ä¼°é¢„æµ‹æ•ˆæœï¼š

```python
# æŒ‰ç”¨æˆ·åˆ†ç»„è®¡ç®—NDCG
def compute_ndcg(group):
    true_ratings = group['true'].tolist()
    pred_ratings = group['pred'].tolist()
    return ndcg_score([true_ratings], [pred_ratings], k=50)

file='.\part2\points.csv'
csv_data = pd.read_csv(file, low_memory = False)
csv_df = pd.DataFrame(csv_data)
ndcg_scores = csv_df.groupby('user').apply(compute_ndcg)
       # è®¡ç®—å¹³å‡NDCG
avg_ndcg = ndcg_scores.mean()
print(
    f'Average NDCG: {avg_ndcg}'
)
```

**ï¼ˆ3ï¼‰ç»“æœåˆ†æ**

![image-20231101103739675](.\figs\image-20231101103739675.png)

![image-20231101103719435](.\figs\image-20231101103719435.png)

![image-20231101104254654](.\figs\image-20231101104254654.png)

ç»è¿‡20è½®çš„è®­ç»ƒï¼Œè®­ç»ƒé›†æ€»æŸå¤±ä»49.78ä¸‹é™åˆ°äº†4.56ï¼Œæµ‹è¯•é›†çš„æ€»æŸå¤±ä»10.07ä¸‹é™åˆ°äº†4.27ï¼Œæœ€åçš„å¹³å‡NDCGè¯„åˆ†ä¸º0.668ã€‚

ç¤ºä¾‹ä»£ç æ•ˆæœï¼šEpoch 19, Train loss: 2.489999907357352, Test loss:, 7.340342143913368, Average NDCG: 0.6925056733687665

å¯ä»¥çœ‹åˆ°ä¸ç¤ºä¾‹ä»£ç å¯¹æ¯”ï¼Œæµ‹è¯•é›†çš„æ€»æŸå¤±ä»7.34ä¸‹é™åˆ°4.27ï¼Œå¹³å‡NDCGè¯„åˆ†ä»0.692ä¸‹é™åˆ°0.668ï¼Œæ¨¡å‹æ•ˆæœæœ‰æ‰€æ”¹å˜ã€‚

åˆæ­¥åˆ†æåŸå› å¯èƒ½æ˜¯æœ‰ä¸€äº›ç”¨æˆ·çš„è¯„åˆ†æ•°æ®è¿‡å°‘ï¼Œç”¨å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œæ¥èšåˆä¿¡æ¯çš„æ•ˆæœæ¯”è¾ƒä¸€èˆ¬ã€‚

## æäº¤æ–‡ä»¶è¯´æ˜

```
â”œâ”€part1 # ç¬¬ä¸€é˜¶æ®µ
â”‚  â”‚  add_tag_to_keyword.py # å°†TagåŠ å…¥å…³é”®è¯
â”‚  â”‚  inverted_index_to_excel.py # åˆ›å»ºå€’æ’è¡¨å¹¶å†™å…¥è¡¨æ ¼
â”‚  â”‚  inverted_index_zip.py # ç´¢å¼•å‹ç¼©,ä»¥åŠé¡ºåºç´¢å¼•å’Œå‹ç¼©ç´¢å¼•çš„æ€§èƒ½å¯¹æ¯”
â”‚  â”‚  jyc.txt # è¿‘ä¹‰è¯
â”‚  â”‚  keyword_search.py # åˆ©ç”¨å€’æ’è¡¨boolæ£€ç´¢
â”‚  â”‚  stop_words.txt # åœç”¨è¯
â”‚  â”‚  test_tyc.py
â”‚  â”‚  test_wordcut.py
â”‚  â”‚  tyc.py # åŒä¹‰è¯
â”‚  â”‚  web_scraper.py # çˆ¬è™«ï¼Œæœé›†ç”µå½±ä¹¦ç±ä¿¡æ¯
â”‚  â”‚  word_cut_jieba.py #jiebaåˆ†è¯
â”‚  â”‚  word_cut_snownlp.py #snownlpåˆ†è¯
â”‚  â”‚
â”‚  â”œâ”€block # åˆ†å—å­˜å‚¨å¾—åˆ°çš„äºŒè¿›åˆ¶æ–‡ä»¶(é‡Œé¢æœ‰250ä¸ªæ–‡ä»¶)
â”‚  â”œâ”€data # ç›¸å…³æ•°æ®
â”‚  â”‚      block_metadata.pkl # å‹ç¼©åçš„å€’æ’è¡¨
â”‚  â”‚      book.xlsx # ä¹¦ç±çš„ç›¸å…³ä¿¡æ¯åŠå…³é”®è¯
â”‚  â”‚      book_com.xlsx # ä¸¤ç§åˆ†è¯çš„æ¯”è¾ƒ
â”‚  â”‚      Book_id.csv # ä¹¦ç±ID
â”‚  â”‚      book_list.xlsx # ä¹¦ç±å€’æ’è¡¨
â”‚  â”‚      Book_tag.csv # ä¹¦ç±Tag
â”‚  â”‚      movie.xlsx # ç”µå½±çš„ç›¸å…³ä¿¡æ¯åŠå…³é”®è¯
â”‚  â”‚      Movie_id.csv # ç”µå½±ID
â”‚  â”‚      movie_list.xlsx # ç”µå½±å€’æ’è¡¨
â”‚  â”‚      Movie_tag.csv # ç”µå½±Tag
â”‚  â”‚
â”‚  â””â”€__pycache__
â”‚          jb.cpython-310.pyc
â”‚          jieba.cpython-310.pyc
â”‚          tyc.cpython-310.pyc
â”‚          word_cut_jieba.cpython-310.pyc
â”‚
â”œâ”€part2
â”‚  â”‚  graphrec.py
â”‚  â”‚  graph_rec_model.py
â”‚  â”‚  text_embedding.py
â”‚  â”‚  utils.py
â”‚  â”‚
â”‚  â”œâ”€data
â”‚  â”‚      book_score.csv
â”‚  â”‚      movie_score.csv
â”‚  â”‚      selected_book_top_1200_data_tag.csv
â”‚  â”‚      selected_movie_top_1200_data_tag.csv
â”‚  â”‚      tag_embedding_dict.pkl
â”‚  â”‚
â”‚  â””â”€__pycache__
â”‚          graph_rec_model.cpython-39.pyc
â”‚          utils.cpython-39.pyc
â”‚
â”œâ”€report
â”‚  â”‚  report.md
â”‚  â”‚
â”‚  â””â”€figs # æŠ¥å‘Šä¸­çš„å›¾åƒ
â”‚
â””â”€useless # ä¸­é—´æ•°æ®ä»¥åŠä¸€äº›æ•°æ®çš„å¤åˆ¶
        book.xlsx
        book_data.xlsx
        movie copy 2.xlsx
        movie_data.xlsx
        test.xlsx
        ~$movie_list.xlsx
```

